{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import sqlalchemy\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "import random\n",
    "import urllib3\n",
    "from nordvpn_connect import initialize_vpn, rotate_VPN, close_vpn_connection\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_ip():\n",
    "    \"\"\" Connects to a nord VPN server.\n",
    "    Args:\n",
    "        No args.\n",
    "    Returns:\n",
    "        Nothing. Changes server in background.\n",
    "    \"\"\" \n",
    "    settings = initialize_vpn(\"Poland\")  # starts nordvpn and stuff\n",
    "    rotate_VPN(settings)  # actually connect to server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def database(url):\n",
    "    \"\"\" Creates context in which engine is created, perform an \n",
    "    action and tear down the connection once finished.\n",
    "    Args:\n",
    "        Connecion URL.\n",
    "    Returns:\n",
    "        Postgres database \n",
    "    \"\"\"\n",
    "    # Create engine\n",
    "    db = sqlalchemy.create_engine(url)\n",
    "    \n",
    "    try: \n",
    "        yield db\n",
    "        \n",
    "    finally:\n",
    "        # Tear down database connection\n",
    "       db.dispose()\n",
    "       # pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_last_page_db():\n",
    "    \"\"\" Check last page in database.\n",
    "    Args:\n",
    "        Connecion URL.\n",
    "    Returns:\n",
    "        Value of the last page added to database.\n",
    "    \"\"\"\n",
    "# set connection\n",
    "    db_string = \"postgresql://postgres:Congitos211!!!@localhost:5432/copart\"\n",
    "\n",
    "    with database(db_string) as db:\n",
    "        # Run the query to fetch the data\n",
    "        result = db.execute(\"SELECT MAX(\\\"Page Number\\\") FROM cars\")\n",
    "        row = result.fetchone() # Select one row\n",
    "        return row[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_null(df):\n",
    "    \"\"\" Check null vales that may appear when scarping is incorrect.\n",
    "    Args:\n",
    "        Dataframe.\n",
    "    Returns:\n",
    "        dataframe without null values and dataframe with null values.\n",
    "    \"\"\"\n",
    "    df_null = df[df.isna().any(axis=1)]\n",
    "    df.dropna(inplace=True)\n",
    "    #if df_null is not None:\n",
    "    #    return df, None\n",
    "    #else:\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## create while loop to download scaping data until no more data is avaliable\n",
    "def extract():\n",
    "        # check last page in DB\n",
    "        page_number = 4969538\n",
    "        #page_number = read_last_page_db()\n",
    "        dict_list = []\n",
    "        loop = 1\n",
    "        while loop == 1:\n",
    "            try:\n",
    "                soup = extract_data_and_create_soup(page_number)\n",
    "                soup_dict = dictionary_with_data_and_features(soup, page_number)\n",
    "                ## Adding data from soup that is not included in the main soup_dict\n",
    "                lot_number(soup_dict, soup)\n",
    "                web_adress(soup_dict)\n",
    "                car_year(soup_dict, soup)\n",
    "                car_model(soup_dict, soup)\n",
    "                dict_list.append(soup_dict)\n",
    "                page_number = page_number + 1\n",
    "            except IndexError:\n",
    "                # In case of being blocked change ip\n",
    "                start = time.time()\n",
    "                change_ip()\n",
    "                end = time.time()\n",
    "                elapsed = end - start\n",
    "                loop = 0\n",
    "                print('ip changed in ' + str(elapsed) + 's')\n",
    "                print('Number of records stored ' + str(len(dict_list)))\n",
    "                df = converting_dictionary_to_dataframe(dict_list)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(df):\n",
    "    ## Change monetary values to correct format\n",
    "    convert_to_num('Final bid:', df)\n",
    "    convert_to_num('Estimated Repair Cost:', df)\n",
    "    convert_to_num('Estimated Retail Value:', df)\n",
    "    location(df)\n",
    "    doc_type(df)\n",
    "    odometer_to_km(df)\n",
    "    convert_date_str_to_date(df)\n",
    "    #engine_type(df)\n",
    "    #cilinders(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load():\n",
    "    # Load\n",
    "    # Load the data in batch processing to sql database and close connectionn\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl():\n",
    "    \"\"\" Scraps data from bids-history.com, create a dataframe with auction detail \n",
    "        and load the data into a sql db.\n",
    "    Args:\n",
    "        None.\n",
    "    Returns:\n",
    "        postgres database \n",
    "    \"\"\"\n",
    "    dictionary = extract()\n",
    "    df = converting_dictionary_to_dataframe(dictionary)\n",
    "    #df = check_for_null(df)\n",
    "    df = transform(df)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def converting_dictionary_to_dataframe(dictionary):\n",
    "    \"\"\" Converts dictionary to dataframe.\n",
    "    Args:\n",
    "        Takes as agrument dict object.\n",
    "    Returns:\n",
    "        Pandas dataframe.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame.from_dict(dictionary,orient='columns')\n",
    "    df = df.set_index(\"Page Number\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_and_create_soup(page_number):\n",
    "    \"\"\" Makes a url request and creates soup.\n",
    "    Args:\n",
    "        None.\n",
    "    Returns:\n",
    "        bs4.BeautifulSoup object.\n",
    "    \"\"\"    \n",
    "    url = \"https://bids-history.com/lot/\" + str(page_number) + \"/\"\n",
    "    #Get the HMTL text from the homepage.\n",
    "    res = requests.get(url,verify = False)\n",
    "    soup = bs4.BeautifulSoup(res.text,'lxml')\n",
    "    # Create soup, if text is 'Not Found' and page_number is high then no more data avaliable \n",
    "    #and stop update. Minimum page number starts from 69600\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary_with_data_and_features(soup, page_number):\n",
    "    \"\"\" Selects features and vales from the soup.\n",
    "    Args:\n",
    "        Takes as agrument object bs4.BeautifulSoup and web page_number.\n",
    "    Returns:\n",
    "        Dictionary with features as keys and data as values. \n",
    "    \"\"\"\n",
    "    items = 0 \n",
    "    Lot_info_key = []\n",
    "    Lot_info_val = []\n",
    "    Lot_info_key.append(\"Page Number\")\n",
    "    Lot_info_val.append(page_number)\n",
    "    for item in soup.select(\".col-6\"):\n",
    "        item = item.text\n",
    "        item = item.replace(\"\\n\", \"\") # Formating word\n",
    "        if items % 2 == 0:\n",
    "            Lot_info_key.append(item)\n",
    "            items += 1\n",
    "        else:\n",
    "            Lot_info_val.append(item)\n",
    "            items += 1\n",
    "    return dict(zip(Lot_info_key, Lot_info_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_num(column_name, df):\n",
    "    \"\"\" Separates data with monetary amounts to 'column_name','currency' columns.\n",
    "    Args:\n",
    "        Takes as agrument column name from dataframe.\n",
    "    Returns:\n",
    "        Nothing. It converts values inplace in the dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    df[[column_name,'Currency']] = df[column_name].str.split(n=1, expand=True) # Formating price\n",
    "    df[column_name].replace(to_replace=[\",\",\"\\$\"],value='',regex=True, inplace=True)\n",
    "    df[column_name] = df[column_name].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    " def odometer_to_km(df):\n",
    "    \"\"\" Converts odometer number to KM.\n",
    "    Args:\n",
    "        No args.\n",
    "    Returns:\n",
    "        Nothing. It converts values inplace in the dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    df['Odometer:'].replace(to_replace=[\",\",\"mi\"],value='',regex=True, inplace=True)\n",
    "    df['Odometer:'] = (df['Odometer:'].astype(int) * 1.60934).astype(int) # miles to KM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_date_str_to_date(df):\n",
    "    \"\"\" Converts string to python date format.\n",
    "    Args:\n",
    "        No args.\n",
    "    Returns:\n",
    "        Nothing. It converts values inplace in the dataframe.\n",
    "    \"\"\"    \n",
    "    # Consolidating months abreviation\n",
    "    df['Auction Date:'] = df['Auction Date:'].str.replace('March', 'Mar')\n",
    "    df['Auction Date:'] = df['Auction Date:'].str.replace('April', 'Apr')\n",
    "    df['Auction Date:'] = df['Auction Date:'].str.replace('June', 'Jun')\n",
    "    df['Auction Date:'] = df['Auction Date:'].str.replace('July', 'Jul')\n",
    "    df['Auction Date:'] = df['Auction Date:'].str.replace('Sept.', 'Sep')\n",
    "    \n",
    "    # Deleting comas and points\n",
    "    df['Auction Date:'] = df['Auction Date:'].str.replace(',', '')\n",
    "    df['Auction Date:'] = df['Auction Date:'].str.replace('.', '')\n",
    "    \n",
    "    # Converting data to date type\n",
    "    df['Auction Date:'] = df['Auction Date:'].str.upper()\n",
    "    df['Auction Date:'] = [datetime.datetime.strptime(date,'%b %d %Y %H %p') for date in df['Auction Date:']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engine_type(car_dict):\n",
    "# Converting Engine Type to float number\n",
    "    car_dict['Engine Type:'] = car_dict['Engine Type:'][0].replace(car_dict['Engine Type:'][0][3:],'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cilinders(car_dict):\n",
    "# Converting cilinders to  float number\n",
    "    car_dict['Cylinders:'] =int(car_dict['Cylinders:'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lot_number(car_dict, soup):\n",
    "    car_dict['Lot number:'] = int(soup.select(\"td a\")[1].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_adress(car_dict):\n",
    "    car_dict['Web adress'] = str(\"https://bids-history.com/lot/\"+ str(car_dict['Page Number']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def car_year(car_dict, soup):\n",
    "    car_dict['Production year'] = soup.select(\"ol li\")[-1].text[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def car_model(car_dict, soup):\n",
    "    car_dict['Car Model'] = soup.select(\"ol li\")[-1].text[5:]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to be corrected form dict to df\n",
    "def location(car_dict):\n",
    "    car_dict['Location'] = car_dict['Doc Type:'][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to be corrected form dict to df\n",
    "def doc_type(car_dict):\n",
    "    car_dict['Doc Type:'] =  car_dict['Doc Type:'][5:] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import sqlalchemy\n",
    "\n",
    "connection_uri = \"postgresql://postgres:Congitos211!!!@localhost:5432/copart\"\n",
    "db_engine_copart = sqlalchemy.create_engine(connection_uri)\n",
    "\n",
    "# Finish the .to_sql() call to write to store.film\n",
    "df.to_sql(\"cars\", con=db_engine_copart,  if_exists=\"replace\")\n",
    "\n",
    "# Run the query to fetch the data\n",
    "pd.read_sql(\"SELECT * FROM cars\", db_engine_copart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-01 00:41:11.092 | INFO     | nordvpn_connect.nordvpn_connect:start_vpn_windows:100 - You're using Windows.\n",
      "Performing system check...\n",
      "\n",
      "2021-02-01 00:41:11.094 | INFO     | nordvpn_connect.nordvpn_connect:start_vpn_windows:120 - NordVPN installation check: OK\n",
      "2021-02-01 00:41:11.263 | INFO     | nordvpn_connect.nordvpn_connect:start_vpn_windows:127 - NordVPN service check: OK\n",
      "2021-02-01 00:41:11.264 | INFO     | nordvpn_connect.nordvpn_connect:start_vpn_windows:130 - Opening NordVPN app and disconnecting if necessary...\n",
      "2021-02-01 00:41:11.479 | INFO     | nordvpn_connect.nordvpn_connect:start_vpn_windows:137 - NordVPN app launched: OK\n",
      "2021-02-01 00:41:11.482 | INFO     | nordvpn_connect.nordvpn_connect:initialize_vpn:71 - Done!\n",
      "2021-02-01 00:41:11.727 | INFO     | nordvpn_connect.nordvpn_connect:check_old_ip:187 - Your current ip-address is: 185.246.208.122\n",
      "2021-02-01 00:41:11.728 | INFO     | nordvpn_connect.nordvpn_connect:rotate_VPN:155 - Connecting you to poland\n",
      "2021-02-01 00:41:26.796 | INFO     | nordvpn_connect.nordvpn_connect:check_ip_changed:169 - Error on api.myip server may still be connecting, waiting 5s ...\n",
      "2021-02-01 00:41:32.137 | INFO     | nordvpn_connect.nordvpn_connect:check_ip_changed:174 - Perfect ! Now new IP 37.120.211.100 is different from old IP 185.246.208.122\n",
      "2021-02-01 00:41:32.138 | INFO     | nordvpn_connect.nordvpn_connect:rotate_VPN:161 - Done! Enjoy your new server.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ip changed in 21.046457529067993s\n",
      "Number of records stored 12\n"
     ]
    }
   ],
   "source": [
    "df = extract() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
